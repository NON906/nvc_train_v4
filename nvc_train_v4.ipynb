{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nvc_train_v4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_KoOiEAlCei"
      },
      "source": [
        "- 上から順に実行してください\n",
        "- ファイルのアップロードは左メニューのフォルダのアイコンから行えます"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqwhmwtehKzb"
      },
      "source": [
        "## 1. 音声ファイルの分割\n",
        "\n",
        "- 全ての音声ファイルに対して行ってください\n",
        "- すでに音声ファイルが分割されている場合は不要です\n",
        "- 複数者の音声が含まれている場合、こちらの実行後に話者ごとに音声ファイルを分けてください"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2PzL7ee1fGF"
      },
      "source": [
        "# Googleドライブに接続\n",
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olU2NpDhh4ug"
      },
      "source": [
        "!pip install inaSpeechSegmenter pydub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KQsmZBwh5ZF"
      },
      "source": [
        "# ---\n",
        "# 設定パラメータ（※実行前に設定してください）\n",
        "\n",
        "# 入力のwavファイルのパス\n",
        "# Googleドライブ内のファイルを指定したい場合は「drive/My Drive/～」としてください\n",
        "# あるいは、ファイルをランタイムにアップロードして、そのファイル名を指定してください\n",
        "input_file = 'test.wav'\n",
        "\n",
        "# 出力のwavファイルのフォルダとプレフィックスまで指定\n",
        "# segment_output/segment000000000.wav、segment_output/test_000000001.wav、のような出力を想定\n",
        "output_file = 'drive/My Drive/segment_output/test_'\n",
        "\n",
        "# ---\n",
        "\n",
        "# 参考：https://tam5917.hatenablog.com/entry/2020/01/25/132113\n",
        "\n",
        "from inaSpeechSegmenter import Segmenter\n",
        "from pydub import AudioSegment\n",
        "\n",
        "seg = Segmenter(vad_engine='smn', detect_gender=False)\n",
        "\n",
        "segmentation = seg(input_file)\n",
        "\n",
        "speech_segment_index = 0\n",
        "for segment in segmentation:\n",
        "    segment_label = segment[0]\n",
        "\n",
        "    if (segment_label == 'speech'):\n",
        "\n",
        "        start_time = segment[1] * 1000\n",
        "        end_time = segment[2] * 1000\n",
        "\n",
        "        newAudio = AudioSegment.from_wav(input_file)\n",
        "        newAudio = newAudio[start_time:end_time]\n",
        "        new_output_file = '{}{:09}.wav'.format(output_file, speech_segment_index)\n",
        "        newAudio.export(new_output_file, format=\"wav\")\n",
        "\n",
        "        speech_segment_index += 1\n",
        "        del newAudio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkKn7gYXj7wy"
      },
      "source": [
        "## 2. 音声の分析処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfuYnJDRlShh"
      },
      "source": [
        "# Googleドライブに接続\n",
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmzQ6ICjseLh"
      },
      "source": [
        "!pip install pysptk pyworld"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9NTmwLM-Dfw"
      },
      "source": [
        "# 解凍が必要な場合（****.zipを対象ファイル名にしてから実行）\r\n",
        "#!unzip ****.zip -d inputs_wav"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI0dTHkd1g32"
      },
      "source": [
        "# ---\n",
        "# 設定パラメータ（※実行前に設定してください）\n",
        "\n",
        "# 入力の音声ファイルのディレクトリのパス\n",
        "# Googleドライブ内のディレクトリを指定したい場合は「drive/My Drive/～」としてください\n",
        "# あるいは、ファイルをランタイムにアップロードして、そのディレクトリ名を指定してください\n",
        "# アップロードの際はzipフォルダにして、それをunzipする方法がおすすめです\n",
        "input_voices_dir = 'inputs_wav'\n",
        "\n",
        "# 音声ファイルの拡張子\n",
        "ext = '.wav'\n",
        "\n",
        "# 出力zipファイル\n",
        "# 「.zip」は省略可\n",
        "output_zip_file = 'drive/My Drive/targets'\n",
        "\n",
        "# 中間ディレクトリ\n",
        "# 通常は変更不要です\n",
        "# 処理に時間がかかる場合は、Googleドライブ内のディレクトリを指定すると、途中までのファイルが保存され、途中から再開することができるようになります\n",
        "gen_dir_name = 'inputs'\n",
        "\n",
        "# ---\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import subprocess\n",
        "import struct\n",
        "import pyworld\n",
        "import numpy as np\n",
        "import pysptk\n",
        "import librosa\n",
        "\n",
        "input_voices = glob.glob(input_voices_dir + '/**/*' + ext, recursive=True)\n",
        "\n",
        "if os.path.isdir(gen_dir_name) == False:\n",
        "    os.mkdir(gen_dir_name)\n",
        "\n",
        "for input_voice_index, input_voice in enumerate(input_voices):\n",
        "    print('\\r音声ファイルの解析中...(' + str(input_voice_index) + '/' + str(len(input_voices)) + ')', end='')\n",
        "\n",
        "    fs = 24000\n",
        "    input_x, _ = librosa.load(input_voice, sr=fs)\n",
        "\n",
        "    for cut in [0, 60]:\n",
        "        for reverse in [False, True]:\n",
        "            name = os.path.basename(input_voice).replace('.', '__') + '_{}_{}'.format(cut, reverse)\n",
        "            if os.path.isfile(gen_dir_name + '/' + name + '.mc'):\n",
        "                continue\n",
        "\n",
        "            x = np.array(input_x[cut:], dtype='float64')\n",
        "            if reverse:\n",
        "                x *= -1\n",
        "          \n",
        "            f0, t = pyworld.harvest(\n",
        "                x,\n",
        "                fs,\n",
        "                frame_period=5.0,\n",
        "                f0_floor=71.0,\n",
        "                f0_ceil=800.0,\n",
        "            )\n",
        "            f0 = pyworld.stonemask(x, f0, t, fs)\n",
        "            \n",
        "            pitch = [0.0 for _ in range(f0.shape[0])]\n",
        "            for loop in range(f0.shape[0]):\n",
        "                if f0[loop] >= 71.0:\n",
        "                    pitch[loop] = fs / f0[loop]\n",
        "                else:\n",
        "                    pitch[loop] = 0.0\n",
        "\n",
        "            write_file = open(gen_dir_name + '/' + name + '.pitch', 'wb')\n",
        "            write_file.write(struct.pack('<' + str(len(pitch)) + 'f', *pitch))\n",
        "            write_file.close()\n",
        "\n",
        "            sp = pyworld.cheaptrick(x, f0, t, fs, fft_size=1024)\n",
        "            \n",
        "            alpha = pysptk.util.mcepalpha(fs)\n",
        "            mc = pysptk.sp2mc(sp, order=32, alpha=alpha)\n",
        "            \n",
        "            write_file = open(gen_dir_name + '/' + name + '.mc', 'wb')\n",
        "            for mc_inner in mc:\n",
        "                write_file.write(struct.pack('<' + str(len(mc_inner)) + 'f', *mc_inner))\n",
        "            write_file.close()\n",
        "\n",
        "print('\\r音声ファイルの解析完了')\n",
        "\n",
        "print('圧縮中...', end='')\n",
        "if output_zip_file[-4:] == '.zip':\n",
        "    output_zip_file = output_zip_file[:-4]\n",
        "shutil.make_archive(output_zip_file, 'zip', root_dir=gen_dir_name)\n",
        "print('\\r圧縮完了')\n",
        "\n",
        "shutil.rmtree(gen_dir_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMvhduaanwx9"
      },
      "source": [
        "## 3. 音源の学習処理\n",
        "\n",
        "- **[こちら](nvc_train_v4_tpu.ipynb)を開いて、そのコードを実行してくだい**\n",
        "- この処理は自動では終了しません。また、再度実行すると途中から再開することができます。「4.」で確認して適宜実行や中断を行ってください"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4YTGkARotnt"
      },
      "source": [
        "## 4. テスト再生"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2g1Ma5oo2uN"
      },
      "source": [
        "# Googleドライブに接続\n",
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXtxv5RtpPAR"
      },
      "source": [
        "!pip install pysptk pyworld\n",
        "\n",
        "!git clone --depth 1 \"https://github.com/NON906/nvc_train_v4.git\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok0HhTWmpazy"
      },
      "source": [
        "# ---\n",
        "# 設定パラメータ（※実行前に設定してください）\n",
        "\n",
        "# 変換元の音声ファイル\n",
        "input_voices = ['nvc_train_v4/test.wav']\n",
        "\n",
        "# 入力音源ファイル\n",
        "model_path = 'drive/My Drive/nvc/gen_000000xx0.h5'\n",
        "\n",
        "# ---\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import pyworld\n",
        "import numpy as np\n",
        "import pysptk\n",
        "import librosa\n",
        "import IPython.display\n",
        "from tensorflow.keras.layers import Dense, LSTM, Lambda, Input, Concatenate\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "phoneme_model_path = 'nvc_train_v4/phoneme.h5'\n",
        "\n",
        "input_layer = Input(shape=(None, 34), name='phoneme_input')\n",
        "\n",
        "f_layers = LSTM(128, return_sequences=True, name='phoneme_lstm0')(input_layer)\n",
        "f_layers = LSTM(128, return_sequences=True, name='phoneme_lstm1')(f_layers)\n",
        "loop_layers = []\n",
        "for loop in range(64):\n",
        "    f_layers_loop = Dense(2, name='phoneme_dense_l' + str(loop))(f_layers)\n",
        "    f_layers_loop = Lambda(lambda x: K.l2_normalize(x, axis=-1), name='phoneme_norm_l' + str(loop))(f_layers_loop)\n",
        "    loop_layers.append(f_layers_loop)\n",
        "phoneme_layers = Concatenate(name='phoneme_concat')(loop_layers)\n",
        "\n",
        "gen_layers_pitch = LSTM(64, return_sequences=True, name='gen_pitch_lstm0')(phoneme_layers)\n",
        "gen_layers_pitch = LSTM(64, return_sequences=True, name='gen_pitch_lstm1')(gen_layers_pitch)\n",
        "gen_layers_pitch = Dense(1, name='gen_pitch_dense')(gen_layers_pitch)\n",
        "\n",
        "concat_layers = Concatenate(name='gen_concat_0')([phoneme_layers, gen_layers_pitch])\n",
        "\n",
        "gen_layers_power = LSTM(64, return_sequences=True, name='gen_power_lstm0')(phoneme_layers)\n",
        "gen_layers_power = LSTM(64, return_sequences=True, name='gen_power_lstm1')(gen_layers_power)\n",
        "gen_layers_power = Dense(1, name='gen_power_dense')(gen_layers_power)\n",
        "\n",
        "gen_layers = LSTM(128, return_sequences=True, name='gen_lstm0')(concat_layers)\n",
        "gen_layers = LSTM(128, return_sequences=True, name='gen_lstm1')(gen_layers)\n",
        "gen_layers = Dense(32, name='gen_dense')(gen_layers)\n",
        "\n",
        "gen_layers = Concatenate(name='gen_concat_1')([gen_layers_power, gen_layers, gen_layers_pitch])\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=gen_layers, name='model')\n",
        "model.load_weights(phoneme_model_path, by_name=True)\n",
        "model.load_weights(model_path, by_name=True)\n",
        "\n",
        "for input_voice in input_voices:\n",
        "    fs = 24000\n",
        "    x, _ = librosa.load(input_voice, sr=fs)\n",
        "    x = np.array(x, dtype='float64')\n",
        "\n",
        "    f0, t = pyworld.harvest(\n",
        "        x,\n",
        "        fs,\n",
        "        frame_period=5.0,\n",
        "        f0_floor=71.0,\n",
        "        f0_ceil=800.0,\n",
        "    )\n",
        "    f0 = pyworld.stonemask(x, f0, t, fs)\n",
        "\n",
        "    pitch = [0.0 for _ in range(f0.shape[0])]\n",
        "    for loop in range(f0.shape[0]):\n",
        "        if f0[loop] >= 71.0:\n",
        "            pitch[loop] = fs / f0[loop]\n",
        "        else:\n",
        "            pitch[loop] = 0.0\n",
        "        pitch[loop] = pitch[loop] / (fs / 71.0) * 4.0\n",
        "    pitch = np.array(pitch, dtype='float64')\n",
        "\n",
        "    sp = pyworld.cheaptrick(x, f0, t, fs, fft_size=1024)\n",
        "\n",
        "    alpha = pysptk.util.mcepalpha(fs)\n",
        "    mc = pysptk.sp2mc(sp, order=32, alpha=alpha)\n",
        "\n",
        "    ap = pyworld.d4c(x, f0, t, fs, fft_size=1024)\n",
        "\n",
        "    input = np.concatenate([mc, pitch[..., np.newaxis]], axis=-1)[np.newaxis, ...]\n",
        "\n",
        "    result = model.predict(input)\n",
        "\n",
        "    synth_f0 = [0.0 for _ in range(f0.shape[0])]\n",
        "    for loop in range(f0.shape[0]):\n",
        "        if result[0, loop, -1] > 0.0:\n",
        "            pitch_result = result[0, loop, -1] * (fs / 71.0) / 4.0\n",
        "            synth_f0[loop] = fs / pitch_result\n",
        "            if synth_f0[loop] > 800.0 or synth_f0[loop] < 71.0:\n",
        "                synth_f0[loop] = 0.0\n",
        "        else:\n",
        "            synth_f0[loop] = 0.0\n",
        "    synth_f0 = np.array(synth_f0, dtype='float64')\n",
        "\n",
        "    synth_sp = pysptk.mc2sp(result[0, :, :-1], alpha, fftlen=1024)\n",
        "    synth_sp = np.array(synth_sp, dtype='float64')\n",
        "\n",
        "    synthesized = pyworld.synthesize(synth_f0, synth_sp, ap, fs)\n",
        "    \n",
        "    print(input_voice + ':')\n",
        "    display(IPython.display.Audio(data=synthesized, rate=fs))\n",
        "\n",
        "K.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5Si-wFRvnSq"
      },
      "source": [
        "## 5. nvc4ファイルの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5NjOJuuJWqz"
      },
      "source": [
        "# Googleドライブに接続\n",
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilUHVt0mnSDa"
      },
      "source": [
        "# ---\n",
        "# 設定パラメータ（※実行前に設定してください）\n",
        "\n",
        "# 入力の解析済みファイル\n",
        "targets_zip_file = 'drive/My Drive/targets.zip'\n",
        "\n",
        "# ---\n",
        "\n",
        "!unzip \"{targets_zip_file}\" -d targets > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8R1uxlLOJd_"
      },
      "source": [
        "# ---\n",
        "# 設定パラメータ（※実行前に設定してください）\n",
        "\n",
        "# 入力音源ファイル\n",
        "input_file = 'drive/My Drive/nvc/gen_000000xx0.h5'\n",
        "\n",
        "# 作成する音源の名前\n",
        "nvc_name = 'サンプル'\n",
        "\n",
        "# 出力ファイル\n",
        "# 「.nvz4」は不要\n",
        "output_nvc_file = 'drive/My Drive/nvc/target'\n",
        "\n",
        "# ---\n",
        "\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Input, Concatenate, Lambda, Activation, Reshape\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "from statistics import mean, variance\n",
        "import json\n",
        "import shutil\n",
        "import uuid\n",
        "import os\n",
        "import glob\n",
        "import struct\n",
        "\n",
        "\n",
        "# ピッチの計算\n",
        "pitch_list = []\n",
        "input_voices = glob.glob('targets/**/*_0_1.00_1.00_False.pitch', recursive=True)\n",
        "for input_voice in input_voices:\n",
        "    read_data = open(input_voice, 'rb').read()\n",
        "    read_array = struct.unpack('<' + str(len(read_data) // 4) + 'f', read_data)\n",
        "    for val in read_array:\n",
        "        if val > 0.0:\n",
        "            pitch_list.append(val)\n",
        "pitch_mean = mean(pitch_list)\n",
        "pitch_variance = variance(pitch_list)\n",
        "\n",
        "\n",
        "# jsonファイルの作成\n",
        "dict_for_json = {\n",
        "    'version' : 1,\n",
        "    'name' : nvc_name,\n",
        "    'uuid' : str(uuid.uuid4()),\n",
        "    'pitch_mean' : pitch_mean,\n",
        "    'pitch_variance' : pitch_variance,\n",
        "}\n",
        "os.makedirs('tmp', exist_ok=True)\n",
        "_ = open('tmp/nvc.json', 'w').write(json.dumps(dict_for_json))\n",
        "\n",
        "\n",
        "# tfliteモデルの作成\n",
        "frame_length = 16\n",
        "\n",
        "# gen\n",
        "output_file = 'tmp/nvc_gen.tflite'\n",
        "\n",
        "input_layer = Input(batch_shape=(1, frame_length, 129), name='gen_input')\n",
        "\n",
        "rec_input = [Input(batch_shape=(1, 128), name='gen_rec_input_0'),\n",
        "             Input(batch_shape=(1, 128), name='gen_rec_input_1'),\n",
        "             Input(batch_shape=(1, 128), name='gen_rec_input_2'),\n",
        "             Input(batch_shape=(1, 128), name='gen_rec_input_3')]\n",
        "\n",
        "rec_output = [None for _ in range(4)]\n",
        "\n",
        "gen_layers, rec_output[0], rec_output[1] = LSTM(128, return_sequences=True, return_state=True, unroll=True, name='gen_lstm0')(input_layer, initial_state=[rec_input[0], rec_input[1]])\n",
        "gen_layers, rec_output[2], rec_output[3] = LSTM(128, return_sequences=True, return_state=True, unroll=True, name='gen_lstm1')(gen_layers, initial_state=[rec_input[2], rec_input[3]])\n",
        "gen_layers = Dense(32, name='gen_dense')(gen_layers)\n",
        "\n",
        "inputs_list = [input_layer] + rec_input\n",
        "outputs_list = [gen_layers] + rec_output\n",
        "model = Model(inputs=inputs_list, outputs=outputs_list, name='gen_model')\n",
        "\n",
        "model.load_weights(input_file, by_name=True)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "_ = open(output_file, 'wb').write(tflite_model)\n",
        "\n",
        "\n",
        "# pitch\n",
        "output_file = 'tmp/nvc_gen_pitch.tflite'\n",
        "\n",
        "input_layer = Input(batch_shape=(1, frame_length, 128), name='gen_input')\n",
        "\n",
        "rec_input = [Input(batch_shape=(1, 64), name='gen_rec_input_0'),\n",
        "             Input(batch_shape=(1, 64), name='gen_rec_input_1'),\n",
        "             Input(batch_shape=(1, 64), name='gen_rec_input_2'),\n",
        "             Input(batch_shape=(1, 64), name='gen_rec_input_3')]\n",
        "\n",
        "rec_output = [None for _ in range(4)]\n",
        "\n",
        "gen_layers, rec_output[0], rec_output[1] = LSTM(64, return_sequences=True, return_state=True, unroll=True, name='gen_pitch_lstm0')(input_layer, initial_state=[rec_input[0], rec_input[1]])\n",
        "gen_layers, rec_output[2], rec_output[3] = LSTM(64, return_sequences=True, return_state=True, unroll=True, name='gen_pitch_lstm1')(gen_layers, initial_state=[rec_input[2], rec_input[3]])\n",
        "gen_layers = Dense(1, name='gen_pitch_dense')(gen_layers)\n",
        "\n",
        "inputs_list = [input_layer] + rec_input\n",
        "outputs_list = [gen_layers] + rec_output\n",
        "model = Model(inputs=inputs_list, outputs=outputs_list, name='gen_pitch_model')\n",
        "\n",
        "model.load_weights(input_file, by_name=True)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "_ = open(output_file, 'wb').write(tflite_model)\n",
        "\n",
        "\n",
        "# power\n",
        "output_file = 'tmp/nvc_gen_power.tflite'\n",
        "\n",
        "input_layer = Input(batch_shape=(1, frame_length, 128), name='gen_input')\n",
        "\n",
        "rec_input = [Input(batch_shape=(1, 64), name='gen_rec_input_0'),\n",
        "             Input(batch_shape=(1, 64), name='gen_rec_input_1'),\n",
        "             Input(batch_shape=(1, 64), name='gen_rec_input_2'),\n",
        "             Input(batch_shape=(1, 64), name='gen_rec_input_3')]\n",
        "\n",
        "rec_output = [None for _ in range(4)]\n",
        "\n",
        "gen_layers, rec_output[0], rec_output[1] = LSTM(64, return_sequences=True, return_state=True, unroll=True, name='gen_power_lstm0')(input_layer, initial_state=[rec_input[0], rec_input[1]])\n",
        "gen_layers, rec_output[2], rec_output[3] = LSTM(64, return_sequences=True, return_state=True, unroll=True, name='gen_power_lstm1')(gen_layers, initial_state=[rec_input[2], rec_input[3]])\n",
        "gen_layers = Dense(1, name='gen_power_dense')(gen_layers)\n",
        "\n",
        "inputs_list = [input_layer] + rec_input\n",
        "outputs_list = [gen_layers] + rec_output\n",
        "model = Model(inputs=inputs_list, outputs=outputs_list, name='gen_power_model')\n",
        "\n",
        "model.load_weights(input_file, by_name=True)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "_ = open(output_file, 'wb').write(tflite_model)\n",
        "\n",
        "\n",
        "# ファイルをまとめる\n",
        "if output_nvc_file[-5:] == '.nvz4':\n",
        "    output_nvc_file = output_nvc_file[:-5]\n",
        "shutil.make_archive(output_nvc_file, 'zip', root_dir='tmp')\n",
        "os.rename(output_nvc_file + '.zip', output_nvc_file + '.nvz4')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}